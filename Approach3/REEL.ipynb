{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "REEL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfBfWLXF4qz3"
      },
      "source": [
        "# Libraries and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Am7C1tfcryk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ffe8ee5-4e56-4a82-bbbc-0951fb29426f"
      },
      "source": [
        "# uncomment for the first run!\n",
        "\n",
        "# !pip install spacy\n",
        "# !pip install spacy-entity-linker\n",
        "# !python -m spacy download en_core_web_sm\n",
        "# !python -m spacy download en_core_web_lg\n",
        "# !python -m spacyEntityLinker \"download_knowledge_base\"\n",
        "\n",
        "# NOTE: Restart the runtime after running this cell."
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy-entity-linker in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy-entity-linker) (1.19.5)\n",
            "Requirement already satisfied: spacy>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy-entity-linker) (3.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (3.7.4.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (3.0.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (8.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (0.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (20.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (2.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (0.4.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (3.0.5)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (0.3.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (2.0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (2.11.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (2.4.1)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (1.7.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (56.1.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->spacy-entity-linker) (0.5.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->spacy-entity-linker) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->spacy-entity-linker) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->spacy-entity-linker) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->spacy-entity-linker) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=3.0.0->spacy-entity-linker) (2.4.7)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy>=3.0.0->spacy-entity-linker) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy>=3.0.0->spacy-entity-linker) (3.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.0.0->spacy-entity-linker) (1.1.1)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.0.0->spacy-entity-linker) (3.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrW41DNp4126"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpIgxE9-4xdI"
      },
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from spacy_entity_linker import EntityLinker\n",
        "import os\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veL4krle44QD"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7ehE5f9gMPI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37c6521f-3cc2-4046-f83f-6ff62fa08839"
      },
      "source": [
        "# Load the data\n",
        "trainData = pd.read_json(\"./train.jsonl\", lines=True, orient=\"records\")\n",
        "valData = pd.read_json(\"./val.jsonl\", lines=True, orient=\"records\")\n",
        "testData = pd.read_json(\"./test.jsonl\", lines=True, orient=\"records\")\n",
        "\n",
        "# Train \n",
        "trainPassages = trainData.passage.values\n",
        "trainQuestions = trainData.question.values\n",
        "trainAnswers = trainData.label.values.astype(int)\n",
        "\n",
        "# Validation\n",
        "valPassages = valData.passage.values\n",
        "valQuestions = valData.question.values\n",
        "valAnswers = valData.label.values.astype(int)\n",
        "\n",
        "# Test\n",
        "testPassages = testData.passage.values\n",
        "testQuestions = testData.question.values\n",
        "\n",
        "\n",
        "# FinalSet\n",
        "finalPassages = np.concatenate((trainPassages, valPassages))\n",
        "finalPassages = np.concatenate((finalPassages, testPassages))\n",
        "\n",
        "finalQuestions = np.concatenate((trainQuestions, valQuestions))\n",
        "finalQuestions = np.concatenate((finalQuestions, testQuestions))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-09 19:57:28,452 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTPfmsn7c0Xa"
      },
      "source": [
        "#Initialize Entity Linker\n",
        "entityLinker = EntityLinker()\n",
        "\n",
        "#initialize language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "#add pipeline\n",
        "nlp.add_pipe(entityLinker, last=True, name=\"entityLinker\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN_Jyss048bH"
      },
      "source": [
        "# Entity Linking & Relation Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJlz-bHdcXmk",
        "outputId": "84077045-48f0-4df3-ae92-95de8cf2cb12"
      },
      "source": [
        "model = on.get_model('wiki80_bert_softmax')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-09 19:58:49,051 - root - INFO - Loading BERT pre-trained checkpoint.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REEirBoIeGu_",
        "outputId": "e00eef02-5140-4751-e22e-066e1ae16d57"
      },
      "source": [
        "model1 = on.get_model('wiki80_bertentity_softmax')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-09 20:02:09,686 - root - INFO - Loading BERT pre-trained checkpoint.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd2moqnOGVJx"
      },
      "source": [
        "def augmentData(questions, passages, labels, file=''):\n",
        "\n",
        "  augData = open(\"Augmented_\" + file + \".jsonl\", 'w')\n",
        "\n",
        "  for question, passage, label in zip(questions, passages, labels):\n",
        "    \n",
        "    qpDict = {}\n",
        "\n",
        "    qpDict['question'] = question\n",
        "\n",
        "    # This gives all the entities present in the question and passage\n",
        "    q = nlp(question)\n",
        "    p = nlp(passage)\n",
        "\n",
        "    #returns all linked entities in the question and passage\n",
        "    qEnts = q._.linkedEntities\n",
        "    pEnts = p._.linkedEntities\n",
        "\n",
        "    if len(qEnts):\n",
        "\n",
        "      for qe in qEnts:\n",
        "        \n",
        "        if qe.get_label() != None:\n",
        "          if qe.get_label() in passage:\n",
        "            qeDes = qe.get_description()\n",
        "          \n",
        "            if qeDes not in passage:\n",
        "              passage = passage + qeDes\n",
        "\n",
        "      qpDict['passage'] = passage\n",
        "      qpDict['label'] = int(label)\n",
        "\n",
        "      augData.write(json.dumps(qpDict) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdCvfmlNuSaY"
      },
      "source": [
        "augmentData(trainQuestions, trainPassages, trainAnswers, 'train')\n",
        "augmentData(valQuestions, valPassages, valAnswers, 'val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WIPJO6soK1K"
      },
      "source": [
        "## Nothing helpful from below\n",
        "\n",
        "BERTSIDE didn't work at all\n",
        "\n",
        "OpenNRE didnt give out any good relations on our dataset.\n",
        "Most of the predicted relations were E1 \"part of\" E2.\n",
        "\n",
        "The third relation extractor was an implementation of matching the blanks paper fine-tuned on SemEval. Didn't give out any good relations on our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZVciRVEuGPU"
      },
      "source": [
        "## OpenNRE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhNsxE6ypDuO"
      },
      "source": [
        "# !git clone https://github.com/thunlp/OpenNRE.git\n",
        "\n",
        "# %cd OpenNRE/\n",
        "# !pip install -r requirements.txt\n",
        "# !python setup.py install \n",
        "\n",
        "# import opennre as on"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxIEj2ZU1rJY"
      },
      "source": [
        "def augmentDataRE():\n",
        "   # print(question)\n",
        "    # print(passage)\n",
        "\n",
        "    # sents = passage.split('.')\n",
        "    # # print(sents)\n",
        "\n",
        "    # # check whether the question has any entities\n",
        "    # if len(qEnts) > 0:\n",
        "\n",
        "    #   # for every entity in question\n",
        "    #   for qe in qEnts:\n",
        "      \n",
        "    #     qe = qe.get_label()\n",
        "    #     qeLen = len(qe)\n",
        "\n",
        "    #     # perform relation extraction of entities present in both question and passage\n",
        "    #     # A entity which is present in both question and passage is used to find relations with all other entities of the passage\n",
        "    #     if qe in passage:\n",
        "\n",
        "    #       # perform sentential relation extraction using every sentence of the passage\n",
        "    #       # for sent in sents:\n",
        "\n",
        "\n",
        "    #       for pe in pEnts:\n",
        "    #         pe = pe.get_label()\n",
        "    #         # if pe in sent and qe in sent and pe != qe:\n",
        "              \n",
        "    #         qeStart = passage.index(qe)\n",
        "    #         qeEnd = qeStart + qeLen - 1\n",
        "\n",
        "    #         peLen = len(pe)\n",
        "    #         if pe in passage and pe != qe:\n",
        "    #           peStart = passage.index(pe)\n",
        "    #           peEnd = peStart + peLen - 1\n",
        "\n",
        "    #           hPos = {'pos': (qeStart, qeEnd)}\n",
        "    #           tPos = {'pos': (peStart, peEnd)}\n",
        "\n",
        "              \n",
        "    #           pred, conf = (model1.infer({'text': passage, 'h': hPos, 't': tPos}))\n",
        "\n",
        "    #           if conf > 0.85:\n",
        "    #             print(\"Qe: %s, Pe: %s\" % (qe,pe))\n",
        "    #             print(pred,conf)\n",
        "    #   break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMPXMleKuIYB"
      },
      "source": [
        "## BERT Relation Extraction using Matching the blanks paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_aI8uIAV328"
      },
      "source": [
        "# !git clone https://github.com/plkmo/BERT-Relation-Extraction.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av4W4CKMV8-O"
      },
      "source": [
        "# !wget https://github.com/sahitya0000/Relation-Classification/raw/master/corpus/SemEval2010_task8_all_data.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "578P0HxHO5Sk"
      },
      "source": [
        "# !unzip SemEval2010_task8_all_data.zip\n",
        "# %rm -rf sample_data/"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-84jC1yWO2f"
      },
      "source": [
        "# !pip install seqeval\n",
        "# !pip install boto3\n",
        "# !pip install filelock\n",
        "# !pip install sentencepiece\n",
        "# %cd BERT-Relation-Extraction/\n",
        "!python main_task.py \\\n",
        "# --train 0 \\\n",
        "--infer 1 \\\n",
        "--num_epochs 3 \\\n",
        "--model_no 0 \\\n",
        "--model_size bert-base-uncased \\\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ0cJA6Pd3q_"
      },
      "source": [
        "import pickle as pk\n",
        "import pandas as pd\n",
        "# import json\n",
        "%cd BERT-Relation-Extraction/\n",
        "argsDict = {}\n",
        "\n",
        "argsDict = {'model_no': 0, 'num_classes': 19, 'model_size': 'bert-base-uncased'}\n",
        "\n",
        "filename = \"./data/args.jsonl\"\n",
        "\n",
        "outfile = open(filename, 'w')\n",
        "outfile.write(json.dumps(argsDict) + '\\n')\n",
        "outfile.close()\n",
        "\n",
        "p = pd.read_json(\"./data/args.jsonl\", lines=True, orient=\"records\")\n",
        "\n",
        "from src.tasks.infer import infer_from_trained\n",
        "inferer = infer_from_trained(args=None, detect_entities=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqgd2mfqqhbI"
      },
      "source": [
        "test2 = \"[E1]University of New Hampshire[/E1] is in [E2]USA[/E2]\"\n",
        "inferer.infer_sentence(test2, detect_entities=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}