{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Custom-neural-model-approach.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwGh1nhDVIDY"
      },
      "source": [
        "*****Installations and imports*****\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh2U_9XQdQpE",
        "outputId": "b4ac7a0b-b037-427f-9370-2c1e1da55439"
      },
      "source": [
        "!pip install torch torchvision\n",
        "!pip install transformers==2.5.1\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install spacy-entity-linker\n",
        "!python -m spacyEntityLinker \"download_knowledge_base\"\n",
        "!pip install torch\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Collecting transformers==2.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1) (2.23.0)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/2e/885bea97567ed610fda71ffb4f9ffc02715589277c6f480ef879dcbe9944/boto3-1.17.45-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 16.8MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 16.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1) (4.41.1)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/e3/5e49e9a83fb605aaa34a1c1173e607302fecae529428c28696fb18f1c2c9/tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 43.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.1) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.1) (2.10)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/14/0b4be62b65c52d6d1c442f24e02d2a9889a73d3c352002e14c70f84a679f/s3transfer-0.3.6-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.8MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.21.0,>=1.20.45\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/d7/efd70bd8a4a4916b8e3b3fe1d5b43bc53d8e43403b1b8beb0747c96e7e75/botocore-1.20.45-py2.py3-none-any.whl (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 44.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.5.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.5.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.5.1) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.45->boto3->transformers==2.5.1) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=cc1d706f48e62d58b6e96f13b380fcb03ca517291efdd59882ebb8bf532a9f58\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: botocore 1.20.45 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed boto3-1.17.45 botocore-1.20.45 jmespath-0.10.0 s3transfer-0.3.6 sacremoses-0.0.44 sentencepiece-0.1.95 tokenizers-0.5.2 transformers-2.5.1\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Collecting spacy-entity-linker\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/28/6c8279e9c4d89128e4bf3fd93b2180d4c74fce2a96e244b258401937c027/spacy_entity_linker-0.0.5-py3-none-any.whl\n",
            "Installing collected packages: spacy-entity-linker\n",
            "Successfully installed spacy-entity-linker-0.0.5\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXgHZfAl0nce"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0hmrvoaivgC",
        "outputId": "9dfcd054-2396-4dfc-dbec-beaaa56e345d"
      },
      "source": [
        "!gsutil cp gs://boolq/train.jsonl .\n",
        "!gsutil cp gs://boolq/dev.jsonl ."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://boolq/train.jsonl...\n",
            "/ [1 files][  6.2 MiB/  6.2 MiB]                                                \n",
            "Operation completed over 1 objects/6.2 MiB.                                      \n",
            "Copying gs://boolq/dev.jsonl...\n",
            "/ [1 files][  2.1 MiB/  2.1 MiB]                                                \n",
            "Operation completed over 1 objects/2.1 MiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwn3O1Kkd9uy"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
        "from spacyEntityLinker import EntityLinker\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQIOeFwyeCjN"
      },
      "source": [
        "# Loading data\n",
        "train_data_df = pd.read_json(\"/content/train.jsonl\", lines=True, orient='records')\n",
        "dev_data_df = pd.read_json(\"/content/dev.jsonl\", lines=True, orient=\"records\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "PhNcMxDeeLD0",
        "outputId": "4790ec05-d459-46e6-eec5-b587f9d5bc6a"
      },
      "source": [
        "dev_data_df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>title</th>\n",
              "      <th>answer</th>\n",
              "      <th>passage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>does ethanol take more energy make that produces</td>\n",
              "      <td>Ethanol fuel</td>\n",
              "      <td>False</td>\n",
              "      <td>All biomass goes through at least some of thes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is house tax and property tax are same</td>\n",
              "      <td>Property tax</td>\n",
              "      <td>True</td>\n",
              "      <td>Property tax or 'house tax' is a local tax on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>is pain experienced in a missing body part or ...</td>\n",
              "      <td>Phantom pain</td>\n",
              "      <td>True</td>\n",
              "      <td>Phantom pain sensations are described as perce...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>is harry potter and the escape from gringotts ...</td>\n",
              "      <td>Harry Potter and the Escape from Gringotts</td>\n",
              "      <td>True</td>\n",
              "      <td>Harry Potter and the Escape from Gringotts is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>is there a difference between hydroxyzine hcl ...</td>\n",
              "      <td>Hydroxyzine</td>\n",
              "      <td>True</td>\n",
              "      <td>Hydroxyzine preparations require a doctor's pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3265</th>\n",
              "      <td>is manic depression the same as bi polar</td>\n",
              "      <td>Bipolar disorder</td>\n",
              "      <td>True</td>\n",
              "      <td>Bipolar disorder, previously known as manic de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3266</th>\n",
              "      <td>was whiskey galore based on a true story</td>\n",
              "      <td>SS Politician</td>\n",
              "      <td>True</td>\n",
              "      <td>SS Politician was an 8000-ton cargo ship owned...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3267</th>\n",
              "      <td>are there plants on the international space st...</td>\n",
              "      <td>Plants in space</td>\n",
              "      <td>True</td>\n",
              "      <td>Plant research continued on the International ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3268</th>\n",
              "      <td>does the hockey puck have to cross the line to...</td>\n",
              "      <td>Goal (ice hockey)</td>\n",
              "      <td>True</td>\n",
              "      <td>In ice hockey, a goal is scored when the puck ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3269</th>\n",
              "      <td>will there be a season 5 of shadowhunters</td>\n",
              "      <td>List of Shadowhunters episodes</td>\n",
              "      <td>False</td>\n",
              "      <td>In April 2017, it was announced that the serie...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3270 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               question  ...                                            passage\n",
              "0      does ethanol take more energy make that produces  ...  All biomass goes through at least some of thes...\n",
              "1                is house tax and property tax are same  ...  Property tax or 'house tax' is a local tax on ...\n",
              "2     is pain experienced in a missing body part or ...  ...  Phantom pain sensations are described as perce...\n",
              "3     is harry potter and the escape from gringotts ...  ...  Harry Potter and the Escape from Gringotts is ...\n",
              "4     is there a difference between hydroxyzine hcl ...  ...  Hydroxyzine preparations require a doctor's pr...\n",
              "...                                                 ...  ...                                                ...\n",
              "3265           is manic depression the same as bi polar  ...  Bipolar disorder, previously known as manic de...\n",
              "3266           was whiskey galore based on a true story  ...  SS Politician was an 8000-ton cargo ship owned...\n",
              "3267  are there plants on the international space st...  ...  Plant research continued on the International ...\n",
              "3268  does the hockey puck have to cross the line to...  ...  In ice hockey, a goal is scored when the puck ...\n",
              "3269          will there be a season 5 of shadowhunters  ...  In April 2017, it was announced that the serie...\n",
              "\n",
              "[3270 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BgeR3-ieTly"
      },
      "source": [
        "passages_train = train_data_df.passage.values\n",
        "questions_train = train_data_df.question.values\n",
        "answers_train = train_data_df.answer.values.astype(int)\n",
        "passages_dev = dev_data_df.passage.values\n",
        "questions_dev = dev_data_df.question.values\n",
        "answers_dev = dev_data_df.answer.values.astype(int)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dyufbmi87fZq",
        "outputId": "c62d07ea-b285-4725-f1f9-5c212580943d"
      },
      "source": [
        "print(len(questions_train))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sD_Dpm9ec-J",
        "outputId": "031c8bec-19de-4018-bddb-3d571e444c30"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5YypkieeGa0"
      },
      "source": [
        "***Loading of train data with entailment***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-W2Dp34e3MS",
        "outputId": "7914657c-a1fd-4360-c37e-0e2504ab66a4"
      },
      "source": [
        "#selecting the important question words that is verb or noun or the starting question word\n",
        "#This is only for selecting the sentences from the passages.\n",
        "array = []\n",
        "for sent in questions_train[0:9427]:\n",
        "    rows = []\n",
        "    question_word =\"\"\n",
        "    question_word = sent.split()[0]\n",
        "    tokenize_word = word_tokenize(sent)\n",
        "    tagged = nltk.pos_tag(tokenize_word)\n",
        "    for word, tag in tagged:\n",
        "      if tag in ('NN', 'VB', 'VBP', 'VBZ'):\n",
        "        rows.append(word)\n",
        "        rows.append(word.capitalize())\n",
        "      \n",
        "    array.append(rows)\n",
        "print(array[3])\n",
        "\n",
        "#This will be used for making features from the questions and passages.\n",
        "array_ques = []\n",
        "for sent in questions_train[0:9427]:\n",
        "    rows = []\n",
        "    question_word =\"\"\n",
        "    question_word = sent.split()[0]\n",
        "    tokenize_word = word_tokenize(sent)\n",
        "    tagged = nltk.pos_tag(tokenize_word)\n",
        "    for word, tag in tagged:\n",
        "      if tag in ('NN', 'VB') or (word == sent.split()[0]):\n",
        "        rows.append(word)\n",
        "        rows.append(word.capitalize())\n",
        "    \n",
        "    array_ques.append(rows)\n",
        "print(array_ques[3])\n",
        "\n",
        "\n",
        "double_passage_line = []\n",
        "for i in range(0,9427):\n",
        "  passage_line = \"\"\n",
        "  sentences = sent_tokenize(passages_train[i])\n",
        "  for sent in sentences:\n",
        "    if any(word in sent for word in array[i]):\n",
        "      passage_line += sent\n",
        "  #print(passage_line)\n",
        "  double_passage_line.append(passage_line)\n",
        "\n",
        "print(len(double_passage_line))\n",
        "\n",
        "rows, cols = (9427, 2) \n",
        "\n",
        "arr=[]\n",
        "for i in range(rows):\n",
        "  row = []\n",
        "  for j in range(cols):\n",
        "    if j==0:\n",
        "      tokenize_word = word_tokenize(questions_train[i])\n",
        "      tagged = nltk.pos_tag(tokenize_word)\n",
        "      row.append(tagged)\n",
        "    else:\n",
        "      #tokenize_word = double_passage_line[i]\n",
        "      tokenize_word = word_tokenize(double_passage_line[i])\n",
        "      #tagged = nltk.pos_tag(tokenize_word)\n",
        "      row.append(tokenize_word)\n",
        "  arr.append(row)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['is', 'Is', 'sugar', 'Sugar', 'sugar', 'Sugar']\n",
            "['is', 'Is', 'sugar', 'Sugar', 'sugar', 'Sugar']\n",
            "9427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd9mhkePgJap",
        "outputId": "c55927de-865b-4694-8825-d9d2fb72502c"
      },
      "source": [
        "\n",
        "#$#\n",
        "#selection of passage lines with the given shortlisted words from question\n",
        "double_passage_line = []\n",
        "for i in range(0,9427):\n",
        "  passage_line = \"\"\n",
        "  sentences = sent_tokenize(passages_train[i])\n",
        "  for sent in sentences:\n",
        "    if any(word in sent for word in array[i]):\n",
        "      passage_line += sent\n",
        "  #print(passage_line)\n",
        "  double_passage_line.append(passage_line)\n",
        "\n",
        "print(len(double_passage_line))\n",
        "  "
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PUGwZ5IgOK2"
      },
      "source": [
        "\n",
        "#$#\n",
        "rows, cols = (9427, 2) \n",
        "\n",
        "arr=[]\n",
        "for i in range(rows):\n",
        "  row = []\n",
        "  for j in range(cols):\n",
        "    if j==0:\n",
        "      tokenize_word = word_tokenize(questions_train[i])\n",
        "      tagged = nltk.pos_tag(tokenize_word)\n",
        "      row.append(tagged)\n",
        "    else:\n",
        "      #tokenize_word = double_passage_line[i]\n",
        "      tokenize_word = word_tokenize(double_passage_line[i])\n",
        "      #tagged = nltk.pos_tag(tokenize_word)\n",
        "      row.append(tokenize_word)\n",
        "  arr.append(row)\n",
        "\n",
        "  #print(arr[0:5])"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDM7hHqBehme"
      },
      "source": [
        "***Features***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjFnZER3hzAa"
      },
      "source": [
        "#function for checking the negative question verb \n",
        "def negation_check(first_word, sentence):\n",
        "  #print(first_word)\n",
        "  #print(sentence)\n",
        "  val = 0\n",
        "  \n",
        "  for word in sentence:\n",
        "    #print(word)\n",
        "    if first_word == \" will \" and (word == \"won't\" or word == \"Won't\") :\n",
        "      #print(\"1\",\"present\")\n",
        "      val = 1\n",
        "      break\n",
        "    elif first_word == \" can \" and (word == \"can't\" or word == \"Can't\") :\n",
        "      #print(\"1\",\"present\")\n",
        "      val = 1\n",
        "      break\n",
        "    \n",
        "    elif first_word == \" do \" and (word == \"don't\" or word == \"Don't\") :\n",
        "      #print(\"1\",\"present\")\n",
        "      val = 1\n",
        "      break\n",
        "  \n",
        "    elif first_word == \" are \" and (word == \"aren't\" or word == \"Aren't\") :\n",
        "      #print(\"1\",\"present\")\n",
        "      val = 1\n",
        "      break\n",
        "\n",
        "    elif first_word == \" does \" and (word == \"doesn't\" or word == \"Doesn't\"):\n",
        "      #print(\"2\",\"present\")\n",
        "      val = 1\n",
        "      break\n",
        "\n",
        "    elif first_word == \" is \" and (word == \"isn't\" or word == \"Isn't\"):\n",
        "      #print(\"3\",\"present\")\n",
        "      val = 1\n",
        "      break\n",
        "\n",
        "    elif first_word == \" was \" and (word == \"wasn't\" or word == \"Wasn't\"):\n",
        "      #print(\"5\",\"present\")\n",
        "      val = 1\n",
        "      break\n",
        "\n",
        "    elif first_word == \" were \" and (word == \"weren't\" or word == \"Weren't\"):\n",
        "      #print(\"6\",\"present\")\n",
        "      val = 1\n",
        "      break\n",
        "\n",
        "    elif word == sentence[len(sentence)-1]:\n",
        "      #print(\"not present\")\n",
        "      val = 0\n",
        "      \n",
        "    else:\n",
        "      continue\n",
        "  return val"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3OtsftAh3S4"
      },
      "source": [
        "#function checking for not in the passage\n",
        "def not_check(sentence):\n",
        "  val = 0\n",
        "  for word in sentence:\n",
        "    \n",
        "    if word == \"not\":\n",
        "      val = 1\n",
        "      break\n",
        "  return val\n"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgKDccHoh6LX"
      },
      "source": [
        "#function checking for verbs that are present in question are there in answer or not\n",
        "def check_for_verb(verb, sentence):\n",
        "  verb_present = 0\n",
        "  for word, tag in verb[1:]:\n",
        "    #print(word, tag)\n",
        "    if tag == 'VBZ' or tag == 'VB' or tag == 'VBP':\n",
        "      if word in sentence:\n",
        "        verb_present = 1\n",
        "        #print(\"verb is there\")\n",
        "        break\n",
        "  return verb_present\n",
        "  #print(sentence)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwnukzLQh9oO"
      },
      "source": [
        "#checking whether last noun in question is present in the passage or not?\n",
        "def check_for_last_noun(last_tuple, sentence):\n",
        "  #print(last_tuple[1])\n",
        "  #print(sentence)\n",
        "  last_noun_present = 0\n",
        "  if last_tuple[1] == 'NN' or last_tuple[1] == 'NNS' or last_tuple[1] == 'NNP':\n",
        "    if last_tuple[0] in sentence:\n",
        "      last_noun_present = 1\n",
        "  return last_noun_present\n",
        "  #print(sentence)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VJjWme2iBaZ"
      },
      "source": [
        "#function checking for similar verbs\n",
        "def check_similar_verb(verb_sent, sentence):\n",
        "  #print(\"verb_sent\", verb_sent)\n",
        "  #print(sentence)\n",
        "  val = 0\n",
        "  for word, tag in verb_sent:\n",
        "    if tag == 'VBP':\n",
        "      synonyms = [] \n",
        "      for syn_set in wordnet.synsets(word): \n",
        "        for l in syn_set.lemmas(): \n",
        "          synonyms.append(l.name()) \n",
        "      if any(word in sentence for word in synonyms):\n",
        "        #print(\"present\")\n",
        "        val = 1\n",
        "  return val\n",
        "        \n"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FydO_4Zby8aY"
      },
      "source": [
        "# Loading the pre-trained BERT model\n",
        "###################################\n",
        "# Embeddings will be derived from\n",
        "# the outputs of this model\n",
        "model = BertModel.from_pretrained('bert-base-uncased',\n",
        "                                  output_hidden_states = True,\n",
        "                                  )\n",
        "\n",
        "# Setting up the tokenizer\n",
        "###################################\n",
        "# This is the same tokenizer that\n",
        "# was used in the model to generate \n",
        "# embeddings to ensure consistency\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5KTQqFteBO_",
        "outputId": "28e7bb0d-3818-4680-e8ff-bb31fd709322"
      },
      "source": [
        "print(questions_train[1])\n",
        "print(passages_train[1])\n",
        "print(answers_train[1])"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "do good samaritan laws protect those who help at an accident\n",
            "Good Samaritan laws offer legal protection to people who give reasonable assistance to those who are, or who they believe to be, injured, ill, in peril, or otherwise incapacitated. The protection is intended to reduce bystanders' hesitation to assist, for fear of being sued or prosecuted for unintentional injury or wrongful death. An example of such a law in common-law areas of Canada: a good Samaritan doctrine is a legal principle that prevents a rescuer who has voluntarily helped a victim in distress from being successfully sued for wrongdoing. Its purpose is to keep people from being reluctant to help a stranger in need for fear of legal repercussions should they make some mistake in treatment. By contrast, a duty to rescue law requires people to offer assistance and holds those who fail to do so liable.\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifbObXE4EZe7"
      },
      "source": [
        "def bert_text_preparation(text, tokenizer):\n",
        "    \"\"\"Preparing the input for BERT\n",
        "    \n",
        "    Takes a string argument and performs\n",
        "    pre-processing like adding special tokens,\n",
        "    tokenization, tokens to ids, and tokens to\n",
        "    segment ids. All tokens are mapped to seg-\n",
        "    ment id = 1.\n",
        "    \n",
        "    Args:\n",
        "        text (str): Text to be converted\n",
        "        tokenizer (obj): Tokenizer object\n",
        "            to convert text into BERT-re-\n",
        "            adable tokens and ids\n",
        "        \n",
        "    Returns:\n",
        "        list: List of BERT-readable tokens\n",
        "        obj: Torch tensor with token ids\n",
        "        obj: Torch tensor segment ids\n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    text = str(text)\n",
        "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    segments_ids = [1]*len(indexed_tokens)\n",
        "\n",
        "    # Convert inputs to PyTorch tensors\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    segments_tensors = torch.tensor([segments_ids])\n",
        "    #print(\"tokenized text\", tokenized_text)\n",
        "    return tokenized_text, tokens_tensor, segments_tensors\n",
        "    \n",
        "def get_bert_embeddings(tokens_tensor, segments_tensors, model):\n",
        "    \"\"\"Get embeddings from an embedding model\n",
        "    \n",
        "    Args:\n",
        "        tokens_tensor (obj): Torch tensor size [n_tokens]\n",
        "            with token ids for each token in text\n",
        "        segments_tensors (obj): Torch tensor size [n_tokens]\n",
        "            with segment ids for each token in text\n",
        "        model (obj): Embedding model to generate embeddings\n",
        "            from token and segment ids\n",
        "    \n",
        "    Returns:\n",
        "        list: List of list of floats of size\n",
        "            [n_tokens, n_embedding_dimensions]\n",
        "            containing embeddings for each token\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    # Gradient calculation id disabled\n",
        "    # Model is in inference mode\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor, segments_tensors)\n",
        "        # Removing the first hidden state\n",
        "        # The first state is the input state\n",
        "        hidden_states = outputs[2][1:]\n",
        "\n",
        "    # Getting embeddings from the final BERT layer\n",
        "    token_embeddings = hidden_states[-1]\n",
        "    # Collapsing the tensor into 1-dimension\n",
        "    token_embeddings = torch.squeeze(token_embeddings, dim=0)\n",
        "    # Converting torchtensors to lists\n",
        "    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
        "    #print('list tokens', list_token_embeddings)\n",
        "    return list_token_embeddings"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6WgEfNBEjDB"
      },
      "source": [
        "def target_embedding(texts):\n",
        "  target_word_embeddings = []\n",
        "  #for text in texts:\n",
        "    #print(text)\n",
        "  for text in texts:\n",
        "    \n",
        "      tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text, tokenizer)\n",
        "    \n",
        "    \n",
        "      list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
        "    \n",
        "      tokenized_texter = []\n",
        "\n",
        "      # Find the position 'bank' in list of tokens\n",
        "      for element in tokenized_text:\n",
        "      \n",
        "        tokenized_texter.append(element.capitalize())\n",
        "\n",
        "      #print(\"text\", text)\n",
        "      if text == None:\n",
        "        continue\n",
        "      else:\n",
        "        text_split = text.split()\n",
        "        f = 0\n",
        "        for i in text_split:\n",
        "          if i in tokenized_text :\n",
        "            f=0\n",
        "            word_index = tokenized_text.index(i)\n",
        "          elif i in tokenized_texter :\n",
        "            f=0\n",
        "            word_index = tokenized_texter.index(i)\n",
        "          else:\n",
        "            f=1\n",
        "            continue\n",
        "        if f==1:\n",
        "          continue\n",
        "    \n",
        "      #print(\"text\", text, \"tokenized_text\", tokenized_text, \"tokens_tensor\", tokens_tensor, \"segments_tensors\", segments_tensors,\"word index\", word_index)\n",
        "      word_embedding = list_token_embeddings[word_index]\n",
        "      target_word_embeddings.append(word_embedding)\n",
        "  return target_word_embeddings\n"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9z-IzjjGcap"
      },
      "source": [
        "entityLinker = EntityLinker()\n",
        "\n",
        "#initialize language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "#add pipeline\n",
        "nlp.add_pipe(entityLinker, last=True, name=\"entityLinker\")\n"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSi9EMKh7HUf"
      },
      "source": [
        "def entity_linked(train_question, train_passage):\n",
        "  \n",
        "  #linking the entity with spacy and returning the percentage of matched ids.\n",
        "  question_entity = []\n",
        "  answer_entity = []\n",
        "  question_entity_names = []\n",
        "  passage_entity_names = []\n",
        "  question = nlp(train_question)#questions_train[1]\n",
        "  passage = nlp(train_passage)\n",
        "  all_linked_entities_question=question._.linkedEntities\n",
        "  all_linked_entities_passage=passage._.linkedEntities\n",
        "  i=0\n",
        "\n",
        "  for sent in question.sents:\n",
        "    j=0\n",
        "    for j in sent._.linkedEntities:\n",
        "      question_entity.append(j.get_id())\n",
        "      #print(\"names\",j.get_label())\n",
        "      question_entity_names.append(j.get_label())\n",
        "\n",
        "  for sent in passage.sents:\n",
        "    j=0\n",
        "    for j in sent._.linkedEntities:\n",
        "      answer_entity.append(j.get_id())\n",
        "      #print(\"names in passages\",j.get_label())\n",
        "      passage_entity_names.append(j.get_label())\n",
        "      passage_entity_names = list(set(passage_entity_names))\n",
        "    #sent._.linkedEntities.pretty_print()\n",
        "\n",
        "  count=0\n",
        "  for i in question_entity:\n",
        "    if i in question_entity and i in answer_entity:\n",
        "      count+=1\n",
        "  if len(question_entity)==0:\n",
        "    percent = 0\n",
        "  else:\n",
        "    percent = count/len(question_entity)\n",
        "  \n",
        "\n",
        "  #using the linked entities and applying bert embeddings on that\n",
        "  texts = passage_entity_names\n",
        "  texting = question_entity_names\n",
        "  question_embedding = target_embedding(texting)\n",
        "  passage_embedding = target_embedding(texts)\n",
        "\n",
        "  # Calculating the distance between the\n",
        "  # embeddings of 'bank' in all the\n",
        "  # given contexts of the word\n",
        "\n",
        "  list_of_distances = []\n",
        "  for text1, embed1 in zip(texting, question_embedding):\n",
        "      #print(\"first loop\",text1,embed1)\n",
        "      for text2, embed2 in zip(texts, passage_embedding):\n",
        "          #print(\"second loop\",text1,embed1)\n",
        "          cos_dist = 1 - cosine(embed1, embed2)\n",
        "          list_of_distances.append([text1,embed1,text2, embed1, cos_dist])\n",
        "\n",
        "  distances_df = pd.DataFrame(list_of_distances, columns=['text1','embedding1','text2','embedding2','similarity'])\n",
        "\n",
        "  similar_list = []\n",
        "  cosine_list = []\n",
        "  similarity_df = distances_df[distances_df.similarity > 0.65]\n",
        "  cosine_list = list(set(similarity_df['text1']))\n",
        "\n",
        "  if len(question_entity_names)==0:\n",
        "    feature_result=0\n",
        "  else:\n",
        "    feature_result = len(cosine_list)/len(question_entity_names) \n",
        "  #feature_result\n",
        "  #print(\"linked\",round(percent,2), \"linked with bert\", round(feature_result,2))\n",
        "  return round(percent,2), round(feature_result,2)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rcs3oPC4iHNy",
        "outputId": "457929a5-49d2-4b27-c724-792e7ffdc1d7"
      },
      "source": [
        "#function for making the data using the given set of features\n",
        "def features(value, index, train_question, train_passage):\n",
        "    \"\"\"Set of four features from lecture\"\"\"\n",
        "    \n",
        "    #print(\"0th value\",value[1])\n",
        "    negation = 0\n",
        "    negation = negation_check(value[0][0][0], value[1])\n",
        "    #feature2 checking for not\n",
        "    not_word = 0\n",
        "    not_word = not_check(value[1])\n",
        "    #checking for VB present in ans or not.\n",
        "    verb_check = 0\n",
        "    verb_check = check_for_verb(value[0], value[1])\n",
        "    #checking for the last noun\n",
        "    last_noun = 0\n",
        "    ques_len = len(value[0])\n",
        "    last_noun = check_for_last_noun(value[0][ques_len-1], value[1])\n",
        "    \n",
        "    #check for similarity between the verbs present in question and answer\n",
        "    similar_verb = 0\n",
        "    if (verb_check==0):\n",
        "      similar_verb = check_similar_verb(value[0], value[1])\n",
        "    #checking for all the nouns present in the passage or not\n",
        "    all_nouns = 0\n",
        "    #linked = entity id's matched percentage\n",
        "    #linked_with_bert = linked entities with bert and cosine similarity percentage\n",
        "    linked, linked_with_bert = entity_linked(train_question, train_passage)\n",
        "\n",
        "    #bias\n",
        "    f1 = 1 if negation is 1 else 0\n",
        "    f2 = 1 if not_word is 1 else 0\n",
        "    f3 = 1 if verb_check is 1 else 0\n",
        "    f4 = 1 if last_noun is 1 else 0\n",
        "    f5 = 1 if similar_verb is 1 else 0\n",
        "    f6 = linked\n",
        "    f7 = linked_with_bert\n",
        "    f8 = 1\n",
        "    return np.array([f1, f2, f3, f4, f5, f6, f7,f8])    \n",
        "\n",
        "\n",
        "features_matrix = [[0,0,0,0,0,0,0,0]]\n",
        "for i in range(5):\n",
        "    a = np.array(features_matrix)\n",
        "    b = np.array([features(arr[i],i, questions_train[i], passages_train[i])])\n",
        "    if i%100 == 0:\n",
        "      print(i)\n",
        "\n",
        "    features_matrix = np.concatenate((a,b), axis=0)\n",
        "features_matrix = features_matrix[1:]\n"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABTz2rgie37O"
      },
      "source": [
        "***Loading the data for test set***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-JD0Bwh690o",
        "outputId": "1cc2e9ac-da98-402a-9d42-9640241d19b9"
      },
      "source": [
        "#selecting the important question words that is verb or noun or the starting question word\n",
        "#This is only for selecting the sentences from the passages.\n",
        "array = []\n",
        "for sent in questions_dev[0:3270]:\n",
        "    rows = []\n",
        "    question_word =\"\"\n",
        "    question_word = sent.split()[0]\n",
        "    tokenize_word = word_tokenize(sent)\n",
        "    tagged = nltk.pos_tag(tokenize_word)\n",
        "    for word, tag in tagged:\n",
        "      if tag in ('NN', 'VB', 'VBP', 'VBZ'):\n",
        "        rows.append(word)\n",
        "        rows.append(word.capitalize())\n",
        "\n",
        "    \n",
        "    array.append(rows)\n",
        "print(array[3])\n",
        "\n",
        "#This will be used for making features from the questions and passages.\n",
        "array_ques = []\n",
        "for sent in questions_dev[0:3270]:\n",
        "    rows = []\n",
        "    question_word =\"\"\n",
        "    question_word = sent.split()[0]\n",
        "    tokenize_word = word_tokenize(sent)\n",
        "    tagged = nltk.pos_tag(tokenize_word)\n",
        "    for word, tag in tagged:\n",
        "      if tag in ('NN', 'VB') or (word == sent.split()[0]):\n",
        "        rows.append(word)\n",
        "        rows.append(word.capitalize())\n",
        "\n",
        "    \n",
        "    array_ques.append(rows)\n",
        "print(array_ques[3])\n",
        "\n",
        "#selection of passage lines with the given shortlisted words from question\n",
        "double_passage_line = []\n",
        "for i in range(0,3270):\n",
        "  passage_line = \"\"\n",
        "  sentences = sent_tokenize(passages_dev[i])\n",
        "  for sent in sentences:\n",
        "    if any(word in sent for word in array[i]):\n",
        "      passage_line += sent\n",
        "  #print(passage_line)\n",
        "  double_passage_line.append(passage_line)\n",
        "\n",
        "print(len(double_passage_line))\n",
        "\n",
        "rows, cols = (3270, 2) \n",
        "\n",
        "arr=[]\n",
        "for i in range(rows):\n",
        "  row = []\n",
        "  for j in range(cols):\n",
        "    if j==0:\n",
        "      tokenize_word = word_tokenize(questions_dev[i])\n",
        "      tagged = nltk.pos_tag(tokenize_word)\n",
        "      row.append(tagged)\n",
        "    else:\n",
        "      #tokenize_word = double_passage_line[i]\n",
        "      tokenize_word = word_tokenize(double_passage_line[i])\n",
        "      #tagged = nltk.pos_tag(tokenize_word)\n",
        "      row.append(tokenize_word)\n",
        "  arr.append(row)\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['is', 'Is', 'potter', 'Potter', 'escape', 'Escape', 'gringotts', 'Gringotts', 'roller', 'Roller', 'coaster', 'Coaster', 'ride', 'Ride']\n",
            "['is', 'Is', 'potter', 'Potter', 'escape', 'Escape', 'gringotts', 'Gringotts', 'roller', 'Roller', 'coaster', 'Coaster', 'ride', 'Ride']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q0VIy3C7DFd",
        "outputId": "cd64af6f-3f90-4e1b-f81d-8b9c1aac9b67"
      },
      "source": [
        "\n",
        "#$#\n",
        "#selection of passage lines with the given shortlisted words from question\n",
        "double_passage_line = []\n",
        "for i in range(0,3270):\n",
        "  passage_line = \"\"\n",
        "  sentences = sent_tokenize(passages_dev[i])\n",
        "  for sent in sentences:\n",
        "    if any(word in sent for word in array[i]):\n",
        "      passage_line += sent\n",
        "  #print(passage_line)\n",
        "  double_passage_line.append(passage_line)\n",
        "\n",
        "print(len(double_passage_line))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K23Mcix60hs"
      },
      "source": [
        "#$#\n",
        "rows, cols = (3270, 2) \n",
        "\n",
        "arr=[]\n",
        "for i in range(rows):\n",
        "  row = []\n",
        "  for j in range(cols):\n",
        "    if j==0:\n",
        "      tokenize_word = word_tokenize(questions_dev[i])\n",
        "      tagged = nltk.pos_tag(tokenize_word)\n",
        "      row.append(tagged)\n",
        "    else:\n",
        "      #tokenize_word = double_passage_line[i]\n",
        "      tokenize_word = word_tokenize(double_passage_line[i])\n",
        "      #tagged = nltk.pos_tag(tokenize_word)\n",
        "      row.append(tokenize_word)\n",
        "  arr.append(row)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znSh3VHVfRYW"
      },
      "source": [
        "***Loading feature matrix for test set***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtjXcxIn5roC",
        "outputId": "dc46c591-1435-425e-d87b-8c6a02968a6c"
      },
      "source": [
        "feature_matrix_test = [[0,0,0,0,0,0,0,0]]\n",
        "for i in range(0,3270):\n",
        "    a = np.array(feature_matrix_test)\n",
        "    b = np.array([features(arr[i],i, questions_dev[i], passages_dev[i])])\n",
        "    if i%100 == 0:\n",
        "      print(i)\n",
        "    feature_matrix_test = np.concatenate((a,b), axis=0)\n",
        "feature_matrix_test = feature_matrix_test[1:]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bypzmkgfpRi"
      },
      "source": [
        "***Bidirectional model without bert embeddings***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iinU_ft-43jT",
        "outputId": "90850f81-1391-4b18-e6f4-716846cc0bf6"
      },
      "source": [
        "#model5 is the Bidirectional Lstm applied with 1 embedding layer, 2 bidirectional layers and 1 output layer\n",
        "inputs5 = keras.Input(shape=(8,), dtype=\"int32\")\n",
        "x5 = layers.Embedding(9427, 8)(inputs5)\n",
        "x5 = layers.Bidirectional(layers.LSTM(50, return_sequences=True))(x5)\n",
        "x5 = layers.Bidirectional(layers.LSTM(50))(x5)\n",
        "outputs5 = layers.Dense(2, activation=\"softmax\")(x5)\n",
        "model5 = keras.Model(inputs5, outputs5)\n",
        "model5.summary()"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 8)]               0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, 8, 8)              75416     \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 8, 100)            23600     \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (None, 100)               60400     \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 159,618\n",
            "Trainable params: 159,618\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tExPoiBy437L"
      },
      "source": [
        "model5.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYOBV8Wc44IO",
        "outputId": "f909b832-d8bd-429f-9dd9-fd67712e34c3"
      },
      "source": [
        "model5.fit(\n",
        "  feature_matrix,\n",
        "  to_categorical(answers_train),\n",
        "  epochs=15,\n",
        "  batch_size=32\n",
        ")"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "295/295 [==============================] - 13s 20ms/step - loss: 0.6656 - accuracy: 0.6208\n",
            "Epoch 2/15\n",
            "295/295 [==============================] - 6s 21ms/step - loss: 0.6642 - accuracy: 0.6184\n",
            "Epoch 3/15\n",
            "295/295 [==============================] - 6s 20ms/step - loss: 0.6621 - accuracy: 0.6209\n",
            "Epoch 4/15\n",
            "295/295 [==============================] - 6s 20ms/step - loss: 0.6597 - accuracy: 0.6210\n",
            "Epoch 5/15\n",
            "295/295 [==============================] - 6s 21ms/step - loss: 0.6579 - accuracy: 0.6250\n",
            "Epoch 6/15\n",
            "295/295 [==============================] - 6s 20ms/step - loss: 0.6587 - accuracy: 0.6238\n",
            "Epoch 7/15\n",
            "295/295 [==============================] - 6s 20ms/step - loss: 0.6570 - accuracy: 0.6246\n",
            "Epoch 8/15\n",
            "295/295 [==============================] - 6s 20ms/step - loss: 0.6590 - accuracy: 0.6181\n",
            "Epoch 9/15\n",
            "295/295 [==============================] - 6s 21ms/step - loss: 0.6560 - accuracy: 0.6226\n",
            "Epoch 10/15\n",
            "295/295 [==============================] - 6s 20ms/step - loss: 0.6562 - accuracy: 0.6246\n",
            "Epoch 11/15\n",
            "295/295 [==============================] - 6s 20ms/step - loss: 0.6576 - accuracy: 0.6153\n",
            "Epoch 12/15\n",
            "295/295 [==============================] - 6s 21ms/step - loss: 0.6571 - accuracy: 0.6230\n",
            "Epoch 13/15\n",
            "295/295 [==============================] - 6s 20ms/step - loss: 0.6561 - accuracy: 0.6207\n",
            "Epoch 14/15\n",
            "295/295 [==============================] - 6s 20ms/step - loss: 0.6587 - accuracy: 0.6183\n",
            "Epoch 15/15\n",
            "295/295 [==============================] - 6s 20ms/step - loss: 0.6578 - accuracy: 0.6191\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f62f2140850>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA-bULh2gmxD",
        "outputId": "22a43dad-927b-4c0d-c34e-25d43e6b07ec"
      },
      "source": [
        "model5.evaluate(\n",
        "  feature_matrix_test,\n",
        "  to_categorical(answers_dev)\n",
        ")"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "103/103 [==============================] - 2s 4ms/step - loss: 0.6566 - accuracy: 0.6297\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6565719842910767, 0.6296635866165161]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDosUVJqgoGu"
      },
      "source": [
        "predictions5 = model5.predict(feature_matrix_test)"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoakbS8ECtPU",
        "outputId": "79f33c3a-c3ee-4d33-ba11-5f7dd956b293"
      },
      "source": [
        "y_pred = np.argmax(predictions5, axis=1)\n",
        "print(y_pred[0:20])\n",
        "print(answers_dev[0:20])"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1]\n",
            "[0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdlsiGTCCtlX",
        "outputId": "3456650e-0c71-4805-9404-f56248eb8021"
      },
      "source": [
        "print(f1_score(answers_dev, y_pred, average='weighted'))\n",
        "print(accuracy_score(answers_dev, y_pred))"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5206346053618371\n",
            "0.6296636085626911\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOliF8BagNfx"
      },
      "source": [
        "***LSTM model without Bert Embeddings***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQogGYH3fzX3",
        "outputId": "c013b209-4c4d-4421-fbb6-944cf7a42324"
      },
      "source": [
        "#model is the model containing 1 embedding layer, 1 lstm layer, 2 dense relu layers, 1 output layer with sigmoid activation function\n",
        "model = keras.Sequential()\n",
        "model.add(layers.Embedding(input_dim=9427, output_dim=64))\n",
        "model.add(layers.LSTM(100))\n",
        "model.add(layers.Dense(20, activation=\"relu\"))\n",
        "model.add(layers.Dense(10, activation=\"relu\"))\n",
        "model.add(layers.Dense(2, activation=\"sigmoid\"))\n",
        "model.summary()"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 64)          603328    \n",
            "_________________________________________________________________\n",
            "lstm_15 (LSTM)               (None, 100)               66000     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 20)                2020      \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                210       \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 2)                 22        \n",
            "=================================================================\n",
            "Total params: 671,580\n",
            "Trainable params: 671,580\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqVHiqF7gA8S"
      },
      "source": [
        "model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amHOh0oZgFeq",
        "outputId": "2fdd9104-8fbb-47dd-eddd-01b2fe3ae8b0"
      },
      "source": [
        "model.fit(\n",
        "  feature_matrix,\n",
        "  to_categorical(answers_train),\n",
        "  epochs=20,\n",
        "  batch_size=16\n",
        ")"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "590/590 [==============================] - 12s 15ms/step - loss: 0.6677 - accuracy: 0.6211\n",
            "Epoch 2/20\n",
            "590/590 [==============================] - 11s 19ms/step - loss: 0.6567 - accuracy: 0.6305\n",
            "Epoch 3/20\n",
            "590/590 [==============================] - 9s 15ms/step - loss: 0.6568 - accuracy: 0.6289\n",
            "Epoch 4/20\n",
            "590/590 [==============================] - 9s 15ms/step - loss: 0.6583 - accuracy: 0.6236\n",
            "Epoch 5/20\n",
            "590/590 [==============================] - 9s 15ms/step - loss: 0.6587 - accuracy: 0.6232\n",
            "Epoch 6/20\n",
            "590/590 [==============================] - 9s 15ms/step - loss: 0.6625 - accuracy: 0.6113\n",
            "Epoch 7/20\n",
            "590/590 [==============================] - 9s 15ms/step - loss: 0.6611 - accuracy: 0.6142\n",
            "Epoch 8/20\n",
            "590/590 [==============================] - 9s 15ms/step - loss: 0.6565 - accuracy: 0.6224\n",
            "Epoch 9/20\n",
            "590/590 [==============================] - 9s 15ms/step - loss: 0.6598 - accuracy: 0.6194\n",
            "Epoch 10/20\n",
            "590/590 [==============================] - 9s 15ms/step - loss: 0.6605 - accuracy: 0.6175\n",
            "Epoch 11/20\n",
            "590/590 [==============================] - 9s 15ms/step - loss: 0.6559 - accuracy: 0.6201\n",
            "Epoch 12/20\n",
            "590/590 [==============================] - 9s 15ms/step - loss: 0.6573 - accuracy: 0.6205\n",
            "Epoch 13/20\n",
            "590/590 [==============================] - 9s 15ms/step - loss: 0.6536 - accuracy: 0.6241\n",
            "Epoch 14/20\n",
            "590/590 [==============================] - 9s 15ms/step - loss: 0.6530 - accuracy: 0.6305\n",
            "Epoch 15/20\n",
            "590/590 [==============================] - 9s 15ms/step - loss: 0.6555 - accuracy: 0.6217\n",
            "Epoch 16/20\n",
            "590/590 [==============================] - 9s 15ms/step - loss: 0.6535 - accuracy: 0.6285\n",
            "Epoch 17/20\n",
            "590/590 [==============================] - 9s 15ms/step - loss: 0.6524 - accuracy: 0.6298\n",
            "Epoch 18/20\n",
            "590/590 [==============================] - 9s 15ms/step - loss: 0.6607 - accuracy: 0.6194\n",
            "Epoch 19/20\n",
            "590/590 [==============================] - 9s 15ms/step - loss: 0.6560 - accuracy: 0.6244\n",
            "Epoch 20/20\n",
            "590/590 [==============================] - 9s 15ms/step - loss: 0.6602 - accuracy: 0.6174\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f62efc49690>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SI8KCQDgbCu",
        "outputId": "dcddb68f-78ad-485d-9380-497aec60b4fd"
      },
      "source": [
        "model.evaluate(\n",
        "  feature_matrix_test,\n",
        "  to_categorical(answers_dev)\n",
        ")"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "103/103 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6217\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.65729159116745, 0.6217125654220581]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aTE4Is2gfaI"
      },
      "source": [
        "predictions = model.predict(feature_matrix_test)"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCBBIXlzBtbw",
        "outputId": "bd2d83af-3e63-4529-e940-73241d64f0ac"
      },
      "source": [
        "y_pred = np.argmax(prediction, axis=1)\n",
        "print(y_pred[0:20])\n",
        "print(answers_dev[0:20])"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1]\n",
            "[0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgE4LkzLBmXS",
        "outputId": "ecb1aaf3-48cc-4aa2-faa1-1330ca10f177"
      },
      "source": [
        "print(f1_score(answers_dev, y_pred, average='weighted'))\n",
        "print(accuracy_score(answers_dev, y_pred))"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4900197078413221\n",
            "0.4850152905198777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuYJnjHBhGDm"
      },
      "source": [
        "***BiLSTM with Bert Embeddings***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYG8Z4GDlhPq"
      },
      "source": [
        "import numpy as np\n",
        "from transformers import AutoTokenizer, pipeline, TFDistilBertModel\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "\n",
        "model_bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "pipe = pipeline('feature-extraction', model=model_bert, tokenizer=tokenizer)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TQfG6X2Es5Y",
        "outputId": "e83df36b-7820-4afb-8e54-04f0abfd27b2"
      },
      "source": [
        "def transformer_embedding(name,inp,model_name):\n",
        "    features = pipe(inp)\n",
        "    features = np.squeeze(features)\n",
        "    return features\n",
        "#z=['The brown fox jumped over the dog','The ship sank in the Atlantic Ocean']\n",
        "x = np.zeros((9427,21,768)) # Make a 10 by 20 by 30 array\n",
        "listing = str(feature_matrix[0])\n",
        "embedding_features1=transformer_embedding('bert-base-uncased',listing,BertModel)\n",
        "x[0] = embedding_features1\n",
        "\n",
        "listing = str(feature_matrix[1])\n",
        "embedding_features1=transformer_embedding('bert-base-uncased',listing,BertModel)\n",
        "x[1] = embedding_features1\n",
        "\n",
        "\n",
        "listing = str(feature_matrix[2])\n",
        "embedding_features1=transformer_embedding('bert-base-uncased',listing,BertModel)\n",
        "if embedding_features1.shape[0]>21:\n",
        "    limit = embedding_features1.shape[0]\n",
        "    for i in range(limit-1,20,-1):\n",
        "      embedding_features1 = np.delete(embedding_features1, i, 0)\n",
        "      print(embedding_features1.shape)\n",
        "x[2] = embedding_features1\n",
        "\n",
        "\n",
        "\n",
        "listing = str(feature_matrix[3])\n",
        "embedding_features1=transformer_embedding('bert-base-uncased',listing,BertModel)\n",
        "shape = np.shape(embedding_features1)\n",
        "if embedding_features1.shape[0]>21:\n",
        " \n",
        "    limit = embedding_features1.shape[0]\n",
        "    \n",
        "    for i in range(limit-1,20,-1):\n",
        "      embedding_features1 = np.delete(embedding_features1, i, 0)\n",
        "elif embedding_features1.shape[0]<21:\n",
        "  padded_array = np.zeros((21, 768))\n",
        "  padded_array[:shape[0],:shape[1]] = embedding_features1\n",
        "    \n",
        "x[3] = padded_array\n",
        "\n",
        "\n",
        "for j in range(4,9427):\n",
        "  listing = str(feature_matrix[j])\n",
        "  embedding_features1=transformer_embedding('bert-base-uncased',listing,BertModel)\n",
        "  shape = np.shape(embedding_features1)\n",
        "  if j%200==0:\n",
        "    print(j)\n",
        "  if embedding_features1.shape[0]>21:\n",
        "      limit = embedding_features1.shape[0]\n",
        "      for i in range(limit-1,20,-1):\n",
        "          embedding_features1 = np.delete(embedding_features1, i, 0)\n",
        "          embedding = embedding_features1\n",
        "  elif embedding_features1.shape[0]<21:\n",
        "      padded_array = np.zeros((21, 768))\n",
        "      padded_array[:shape[0],:shape[1]] = embedding_features1\n",
        "      embedding = padded_array\n",
        "  else:\n",
        "      embedding = embedding_features1\n",
        "      \n",
        "  x[j] = embedding\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21, 768)\n",
            "200\n",
            "400\n",
            "600\n",
            "800\n",
            "1000\n",
            "1200\n",
            "1400\n",
            "1600\n",
            "1800\n",
            "2000\n",
            "2200\n",
            "2400\n",
            "2600\n",
            "2800\n",
            "3000\n",
            "3200\n",
            "3400\n",
            "3600\n",
            "3800\n",
            "4000\n",
            "4200\n",
            "4400\n",
            "4600\n",
            "4800\n",
            "5000\n",
            "5200\n",
            "5400\n",
            "5600\n",
            "5800\n",
            "6000\n",
            "6200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivbAyApK0RlM",
        "outputId": "2a4eca7d-1edd-418d-e3e7-181554722a91"
      },
      "source": [
        "y = np.zeros((3270,21,768))\n",
        "\n",
        "listing = str(feature_matrix_test[0])\n",
        "embedding_features1=transformer_embedding('bert-base-uncased',listing,BertModel)\n",
        "shape = np.shape(embedding_features1)\n",
        "#if j%200==0:\n",
        "    #print(j)\n",
        "if embedding_features1.shape[0]>21:\n",
        "    limit = embedding_features1.shape[0]\n",
        "    for i in range(limit-1,20,-1):\n",
        "        embedding_features1 = np.delete(embedding_features1, i, 0)\n",
        "        embedding = embedding_features1\n",
        "elif embedding_features1.shape[0]<21:\n",
        "    padded_array = np.zeros((21, 768))\n",
        "    padded_array[:shape[0],:shape[1]] = embedding_features1\n",
        "    embedding = padded_array\n",
        "else:\n",
        "    embedding = embedding_features1\n",
        "      \n",
        "y[0] = embedding\n",
        "\n",
        "\n",
        "for j in range(1,3270):\n",
        "  listing = str(feature_matrix_test[j])\n",
        "  embedding_features1=transformer_embedding('bert-base-uncased',listing,BertModel)\n",
        "  shape = np.shape(embedding_features1)\n",
        "  if j%200==0:\n",
        "    print(j)\n",
        "  if embedding_features1.shape[0]>21:\n",
        "      limit = embedding_features1.shape[0]\n",
        "      for i in range(limit-1,20,-1):\n",
        "          embedding_features1 = np.delete(embedding_features1, i, 0)\n",
        "          embedding = embedding_features1\n",
        "  elif embedding_features1.shape[0]<21:\n",
        "      padded_array = np.zeros((21, 768))\n",
        "      padded_array[:shape[0],:shape[1]] = embedding_features1\n",
        "      embedding = padded_array\n",
        "  else:\n",
        "      embedding = embedding_features1\n",
        "      \n",
        "  y[j] = embedding"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n",
            "400\n",
            "600\n",
            "800\n",
            "1000\n",
            "1200\n",
            "1400\n",
            "1600\n",
            "1800\n",
            "2000\n",
            "2200\n",
            "2400\n",
            "2600\n",
            "2800\n",
            "3000\n",
            "3200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v47Ute98hUc1"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(12, input_shape=( 21,768), return_sequences=True)))\n",
        "model.add(LSTM(12, return_sequences=True))\n",
        "model.add(LSTM(6, return_sequences=False))\n",
        "model.add((Dense((2))))"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFaVrjNuhbKZ",
        "outputId": "ece07e97-de0f-4878-f2c6-966f2b58961a"
      },
      "source": [
        "#model.compile(optimizer='sgd', loss='mse')\n",
        "model.compile(optimizer='sgd', loss ='mse', metrics=['accuracy'])\n",
        "# This builds the model for the first time:\n",
        "model.fit(x, answers_train, batch_size=32, epochs=10)"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "295/295 [==============================] - 18s 35ms/step - loss: 0.2636 - accuracy: 0.4599\n",
            "Epoch 2/10\n",
            "295/295 [==============================] - 10s 35ms/step - loss: 0.2354 - accuracy: 0.5521\n",
            "Epoch 3/10\n",
            "295/295 [==============================] - 10s 35ms/step - loss: 0.2334 - accuracy: 0.5481\n",
            "Epoch 4/10\n",
            "295/295 [==============================] - 10s 35ms/step - loss: 0.2347 - accuracy: 0.5148\n",
            "Epoch 5/10\n",
            "295/295 [==============================] - 10s 35ms/step - loss: 0.2363 - accuracy: 0.4856\n",
            "Epoch 6/10\n",
            "295/295 [==============================] - 10s 36ms/step - loss: 0.2338 - accuracy: 0.4928\n",
            "Epoch 7/10\n",
            "295/295 [==============================] - 10s 35ms/step - loss: 0.2343 - accuracy: 0.4842\n",
            "Epoch 8/10\n",
            "295/295 [==============================] - 10s 35ms/step - loss: 0.2310 - accuracy: 0.4934\n",
            "Epoch 9/10\n",
            "295/295 [==============================] - 10s 35ms/step - loss: 0.2338 - accuracy: 0.4805\n",
            "Epoch 10/10\n",
            "295/295 [==============================] - 10s 35ms/step - loss: 0.2353 - accuracy: 0.4889\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f62f23f3890>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlL0VqdD9FKg"
      },
      "source": [
        "prediction = model.predict(y)"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_7xBSAp9pqd",
        "outputId": "36ab1c8f-a6ae-42de-efd1-d03ae9ab56d9"
      },
      "source": [
        "y_pred = np.argmax(prediction, axis=1)\n",
        "print(y_pred[0:20])\n",
        "print(answers_dev[0:20])"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1]\n",
            "[0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEE2Vb3hAqxd",
        "outputId": "340bc901-3836-4ab1-a3f2-291a1f165c73"
      },
      "source": [
        "print(f1_score(answers_dev, y_pred, average='weighted'))\n",
        "print(accuracy_score(answers_dev, y_pred))"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4900197078413221\n",
            "0.4850152905198777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2d5s0ar-l8d"
      },
      "source": [
        "***LSTM with embeddings***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KAAxBHqK3tq"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(12, input_shape=( 21,768), return_sequences=True))\n",
        "model.add(LSTM(12, return_sequences=True))\n",
        "model.add(LSTM(6, return_sequences=False))\n",
        "model.add((Dense((2))))"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGaalg6mNlkR",
        "outputId": "dd667749-bda4-4f7b-e554-8d6eca5fa511"
      },
      "source": [
        "#model.compile(optimizer='sgd', loss='mse')\n",
        "model.compile(optimizer='adam', loss ='binary_crossentropy', metrics=['accuracy'])\n",
        "# This builds the model for the first time:\n",
        "model.fit(x, answers_train, batch_size=16, epochs=10)"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "590/590 [==============================] - 20s 25ms/step - loss: 0.8416 - accuracy: 0.5098\n",
            "Epoch 2/10\n",
            "590/590 [==============================] - 15s 25ms/step - loss: 0.6624 - accuracy: 0.5096\n",
            "Epoch 3/10\n",
            "590/590 [==============================] - 15s 25ms/step - loss: 0.6615 - accuracy: 0.5116\n",
            "Epoch 4/10\n",
            "590/590 [==============================] - 16s 26ms/step - loss: 0.6639 - accuracy: 0.5084\n",
            "Epoch 5/10\n",
            "590/590 [==============================] - 15s 26ms/step - loss: 0.6646 - accuracy: 0.5033\n",
            "Epoch 6/10\n",
            "590/590 [==============================] - 15s 26ms/step - loss: 0.6674 - accuracy: 0.4942\n",
            "Epoch 7/10\n",
            "590/590 [==============================] - 15s 26ms/step - loss: 0.6632 - accuracy: 0.4995\n",
            "Epoch 8/10\n",
            "590/590 [==============================] - 16s 26ms/step - loss: 0.6650 - accuracy: 0.4948\n",
            "Epoch 9/10\n",
            "590/590 [==============================] - 16s 26ms/step - loss: 0.6632 - accuracy: 0.4902\n",
            "Epoch 10/10\n",
            "590/590 [==============================] - 16s 27ms/step - loss: 0.6633 - accuracy: 0.5177\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f62f4272710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAgkcYPBQ6uS"
      },
      "source": [
        "prediction = model.predict(y)"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LATXzFxwAQRF",
        "outputId": "aeba49bc-03e5-4559-dcb3-ff8f832101f1"
      },
      "source": [
        "print(f1_score(answers_dev, y_pred, average='weighted'))\n",
        "print(accuracy_score(answers_dev, y_pred))"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.382146790008909\n",
            "0.4235474006116208\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}